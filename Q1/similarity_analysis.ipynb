{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Similarity Analysis - VidyaVichar MERN Projects\n",
    "\n",
    "**Objective**: Analyze code similarity across multiple implementations of the VidyaVichar MERN project using textual, structural, and semantic similarity metrics.\n",
    "\n",
    "**Author**: Assignment 3 - Q1  \n",
    "**Date**: November 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup & Installation](#setup)\n",
    "2. [Part A: Preprocessing & Data Understanding](#part-a)\n",
    "3. [Part B: Similarity Computation](#part-b)\n",
    "4. [Part C: Visualization & Analysis](#part-c)\n",
    "5. [Conclusions](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation <a name=\"setup\"></a>\n",
    "\n",
    "Install required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install python-Levenshtein scikit-learn transformers matplotlib seaborn plotly networkx esprima torch numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import difflib\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import Levenshtein\n",
    "import esprima\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Part A: Preprocessing & Data Understanding <a name=\"part-a\"></a>\n",
    "\n",
    "### 2.1 Project Organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "PROJECTS_DIR = \"projects\"  # Directory containing all 27 projects\n",
    "RESULTS_DIR = \"results\"\n",
    "VALID_EXTENSIONS = ('.js', '.jsx', '.json', '.css')\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Get list of project directories\n",
    "if os.path.exists(PROJECTS_DIR):\n",
    "    project_dirs = sorted([d for d in os.listdir(PROJECTS_DIR) \n",
    "                          if os.path.isdir(os.path.join(PROJECTS_DIR, d))])\n",
    "    print(f\"Found {len(project_dirs)} projects:\")\n",
    "    for i, proj in enumerate(project_dirs, 1):\n",
    "        print(f\"  {i}. {proj}\")\n",
    "else:\n",
    "    print(f\"⚠ Projects directory not found: {PROJECTS_DIR}\")\n",
    "    print(\"Please create 'projects/' directory and add all 27 VidyaVichar project folders\")\n",
    "    project_dirs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_js_comments(code: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove single-line and multi-line comments from JavaScript code.\n",
    "    \n",
    "    Args:\n",
    "        code: JavaScript source code string\n",
    "        \n",
    "    Returns:\n",
    "        Code with comments removed\n",
    "    \"\"\"\n",
    "    # Remove multi-line comments /* ... */\n",
    "    code = re.sub(r'/\\*.*?\\*/', '', code, flags=re.DOTALL)\n",
    "    # Remove single-line comments //\n",
    "    code = re.sub(r'//.*?$', '', code, flags=re.MULTILINE)\n",
    "    return code\n",
    "\n",
    "\n",
    "def is_minified(code: str) -> bool:\n",
    "    \"\"\"\n",
    "    Detect if code is minified (very long lines, no whitespace).\n",
    "    \n",
    "    Args:\n",
    "        code: Source code string\n",
    "        \n",
    "    Returns:\n",
    "        True if code appears to be minified\n",
    "    \"\"\"\n",
    "    lines = code.split('\\n')\n",
    "    if not lines:\n",
    "        return False\n",
    "    \n",
    "    # Check average line length\n",
    "    avg_line_length = sum(len(line) for line in lines) / len(lines)\n",
    "    \n",
    "    # Minified files typically have very long lines (> 500 chars)\n",
    "    # and few newlines\n",
    "    return avg_line_length > 500 or (len(lines) < 10 and len(code) > 1000)\n",
    "\n",
    "\n",
    "def normalize_formatting(code: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize indentation and spacing.\n",
    "    \n",
    "    Args:\n",
    "        code: Source code string\n",
    "        \n",
    "    Returns:\n",
    "        Normalized code\n",
    "    \"\"\"\n",
    "    # Split into lines and remove leading/trailing whitespace\n",
    "    lines = [line.strip() for line in code.split('\\n')]\n",
    "    # Remove empty lines\n",
    "    lines = [line for line in lines if line]\n",
    "    # Normalize multiple spaces to single space\n",
    "    lines = [re.sub(r'\\s+', ' ', line) for line in lines]\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "\n",
    "def preprocess_code(code: str, file_ext: str) -> str:\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline for code.\n",
    "    \n",
    "    Args:\n",
    "        code: Source code string\n",
    "        file_ext: File extension (.js, .jsx, etc.)\n",
    "        \n",
    "    Returns:\n",
    "        Preprocessed code or empty string if minified\n",
    "    \"\"\"\n",
    "    # Skip minified files\n",
    "    if is_minified(code):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove comments for JS/JSX files\n",
    "    if file_ext in ('.js', '.jsx'):\n",
    "        code = remove_js_comments(code)\n",
    "    \n",
    "    # Normalize formatting\n",
    "    code = normalize_formatting(code)\n",
    "    \n",
    "    return code\n",
    "\n",
    "\n",
    "print(\"✓ Preprocessing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Project Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_react_components(file_path: str) -> int:\n",
    "    \"\"\"\n",
    "    Count React components in a JS/JSX file.\n",
    "    Looks for: function components, class components, arrow functions returning JSX.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to JS/JSX file\n",
    "        \n",
    "    Returns:\n",
    "        Number of React components found\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        count = 0\n",
    "        # Class components: class X extends React.Component or Component\n",
    "        count += len(re.findall(r'class\\s+\\w+\\s+extends\\s+(React\\.)?Component', content))\n",
    "        # Function components: function X() { return (...JSX...) }\n",
    "        count += len(re.findall(r'function\\s+[A-Z]\\w*\\s*\\([^)]*\\)\\s*{[^}]*return\\s*\\(', content))\n",
    "        # Arrow function components: const X = () => (...)\n",
    "        count += len(re.findall(r'const\\s+[A-Z]\\w*\\s*=\\s*\\([^)]*\\)\\s*=>\\s*[\\({]', content))\n",
    "        \n",
    "        return count\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def count_express_routes(file_path: str) -> int:\n",
    "    \"\"\"\n",
    "    Count Express routes in a JS file.\n",
    "    Looks for: app.get, app.post, router.get, router.post, etc.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to JS file\n",
    "        \n",
    "    Returns:\n",
    "        Number of Express routes found\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Match app.METHOD or router.METHOD\n",
    "        pattern = r'(app|router)\\.(get|post|put|delete|patch)\\s*\\('\n",
    "        return len(re.findall(pattern, content))\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def count_mongoose_models(file_path: str) -> int:\n",
    "    \"\"\"\n",
    "    Count Mongoose model definitions in a JS file.\n",
    "    Looks for: mongoose.Schema, mongoose.model patterns.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to JS file\n",
    "        \n",
    "    Returns:\n",
    "        Number of Mongoose models found\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        count = 0\n",
    "        # mongoose.Schema\n",
    "        count += len(re.findall(r'new\\s+mongoose\\.Schema\\s*\\(', content))\n",
    "        # mongoose.model\n",
    "        count += len(re.findall(r'mongoose\\.model\\s*\\(', content))\n",
    "        \n",
    "        return count\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def analyze_project(project_path: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze a project directory and extract metrics.\n",
    "    \n",
    "    Args:\n",
    "        project_path: Path to project directory\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing project metrics\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'name': os.path.basename(project_path),\n",
    "        'total_files': 0,\n",
    "        'total_folders': 0,\n",
    "        'loc': 0,\n",
    "        'react_components': 0,\n",
    "        'express_routes': 0,\n",
    "        'mongoose_models': 0,\n",
    "        'files_by_type': {ext: 0 for ext in VALID_EXTENSIONS},\n",
    "        'all_code': []  # Store all preprocessed code for similarity analysis\n",
    "    }\n",
    "    \n",
    "    for root, dirs, files in os.walk(project_path):\n",
    "        # Skip node_modules and hidden directories\n",
    "        dirs[:] = [d for d in dirs if not d.startswith('.') and d != 'node_modules']\n",
    "        \n",
    "        metrics['total_folders'] += len(dirs)\n",
    "        \n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            file_ext = os.path.splitext(file)[1]\n",
    "            \n",
    "            if file_ext in VALID_EXTENSIONS:\n",
    "                metrics['total_files'] += 1\n",
    "                metrics['files_by_type'][file_ext] += 1\n",
    "                \n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                        content = f.read()\n",
    "                    \n",
    "                    # Count LOC\n",
    "                    lines = content.split('\\n')\n",
    "                    metrics['loc'] += len([l for l in lines if l.strip()])\n",
    "                    \n",
    "                    # Preprocess and store code\n",
    "                    preprocessed = preprocess_code(content, file_ext)\n",
    "                    if preprocessed:\n",
    "                        metrics['all_code'].append(preprocessed)\n",
    "                    \n",
    "                    # Count React components\n",
    "                    if file_ext in ('.js', '.jsx'):\n",
    "                        metrics['react_components'] += count_react_components(file_path)\n",
    "                        metrics['express_routes'] += count_express_routes(file_path)\n",
    "                        metrics['mongoose_models'] += count_mongoose_models(file_path)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "print(\"✓ Project analysis functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Analyze All Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze all projects\n",
    "project_metrics = []\n",
    "\n",
    "if project_dirs:\n",
    "    print(\"Analyzing projects...\\n\")\n",
    "    for proj_name in project_dirs:\n",
    "        proj_path = os.path.join(PROJECTS_DIR, proj_name)\n",
    "        print(f\"Analyzing: {proj_name}\")\n",
    "        metrics = analyze_project(proj_path)\n",
    "        project_metrics.append(metrics)\n",
    "        print(f\"  Files: {metrics['total_files']}, LOC: {metrics['loc']}, \"\n",
    "              f\"Components: {metrics['react_components']}, Routes: {metrics['express_routes']}\")\n",
    "    \n",
    "    print(f\"\\n✓ Analyzed {len(project_metrics)} projects\")\n",
    "else:\n",
    "    print(\"⚠ No projects to analyze. Please add projects to 'projects/' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Generate Preprocessing Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if project_metrics:\n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    for m in project_metrics:\n",
    "        summary_data.append({\n",
    "            'Project': m['name'],\n",
    "            'Total Files': m['total_files'],\n",
    "            'Total Folders': m['total_folders'],\n",
    "            'Lines of Code': m['loc'],\n",
    "            'React Components': m['react_components'],\n",
    "            'Express Routes': m['express_routes'],\n",
    "            'Mongoose Models': m['mongoose_models'],\n",
    "            'JS Files': m['files_by_type'].get('.js', 0),\n",
    "            'JSX Files': m['files_by_type'].get('.jsx', 0),\n",
    "            'JSON Files': m['files_by_type'].get('.json', 0),\n",
    "            'CSS Files': m['files_by_type'].get('.css', 0),\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Save to CSV\n",
    "    summary_df.to_csv(os.path.join(RESULTS_DIR, 'preprocessing_summary.csv'), index=False)\n",
    "    \n",
    "    # Display summary\n",
    "    print(\"\\n=== PROJECT SUMMARY ===\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    # Statistics\n",
    "    print(\"\\n=== STATISTICS ===\")\n",
    "    print(summary_df.describe())\n",
    "    \n",
    "    print(f\"\\n✓ Saved preprocessing summary to {RESULTS_DIR}/preprocessing_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Part B: Similarity Computation <a name=\"part-b\"></a>\n",
    "\n",
    "### 3.1 Textual Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_textual_similarity(project_metrics: List[Dict]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute textual similarity using TF-IDF and cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "        project_metrics: List of project metric dictionaries\n",
    "        \n",
    "    Returns:\n",
    "        NxN similarity matrix\n",
    "    \"\"\"\n",
    "    n = len(project_metrics)\n",
    "    \n",
    "    # Combine all code from each project\n",
    "    project_texts = []\n",
    "    for metrics in project_metrics:\n",
    "        combined_text = ' '.join(metrics['all_code'])\n",
    "        project_texts.append(combined_text)\n",
    "    \n",
    "    # Compute TF-IDF vectors\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=5000,\n",
    "        ngram_range=(1, 2),  # Use unigrams and bigrams\n",
    "        min_df=1\n",
    "    )\n",
    "    \n",
    "    tfidf_matrix = vectorizer.fit_transform(project_texts)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "def compute_levenshtein_similarity(project_metrics: List[Dict]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute token-level similarity using Levenshtein distance.\n",
    "    \n",
    "    Args:\n",
    "        project_metrics: List of project metric dictionaries\n",
    "        \n",
    "    Returns:\n",
    "        NxN similarity matrix (normalized)\n",
    "    \"\"\"\n",
    "    n = len(project_metrics)\n",
    "    similarity_matrix = np.zeros((n, n))\n",
    "    \n",
    "    # Sample a subset of code for performance (first 10000 chars)\n",
    "    project_samples = []\n",
    "    for metrics in project_metrics:\n",
    "        combined = ''.join(metrics['all_code'][:100])  # First 100 files\n",
    "        sample = combined[:10000]  # First 10k characters\n",
    "        project_samples.append(sample)\n",
    "    \n",
    "    # Compute pairwise Levenshtein similarity\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                similarity_matrix[i][j] = 1.0\n",
    "            else:\n",
    "                # Levenshtein ratio (0 to 1, where 1 is identical)\n",
    "                ratio = Levenshtein.ratio(project_samples[i], project_samples[j])\n",
    "                similarity_matrix[i][j] = ratio\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "print(\"✓ Textual similarity functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute textual similarity\n",
    "if project_metrics:\n",
    "    print(\"Computing textual similarity...\")\n",
    "    \n",
    "    # TF-IDF based similarity\n",
    "    textual_similarity = compute_textual_similarity(project_metrics)\n",
    "    print(f\"  TF-IDF similarity matrix: {textual_similarity.shape}\")\n",
    "    \n",
    "    # Save matrix\n",
    "    np.save(os.path.join(RESULTS_DIR, 'textual_similarity_matrix.npy'), textual_similarity)\n",
    "    \n",
    "    # Save as CSV for readability\n",
    "    project_names = [m['name'] for m in project_metrics]\n",
    "    textual_df = pd.DataFrame(textual_similarity, index=project_names, columns=project_names)\n",
    "    textual_df.to_csv(os.path.join(RESULTS_DIR, 'textual_similarity_matrix.csv'))\n",
    "    \n",
    "    print(\"✓ Textual similarity computed and saved\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(\"\\nSample (first 5x5):\")\n",
    "    print(textual_df.iloc[:5, :5].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Structural Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ast_features(file_path: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract AST features from a JavaScript file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to JS/JSX file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of AST features\n",
    "    \"\"\"\n",
    "    features = {\n",
    "        'function_count': 0,\n",
    "        'class_count': 0,\n",
    "        'import_count': 0,\n",
    "        'export_count': 0,\n",
    "        'variable_count': 0,\n",
    "        'node_types': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            code = f.read()\n",
    "        \n",
    "        # Parse AST\n",
    "        ast = esprima.parseModule(code, {'jsx': True, 'tolerant': True})\n",
    "        \n",
    "        # Traverse AST and collect features\n",
    "        def traverse(node):\n",
    "            if isinstance(node, dict):\n",
    "                node_type = node.get('type', '')\n",
    "                features['node_types'].append(node_type)\n",
    "                \n",
    "                if 'Function' in node_type:\n",
    "                    features['function_count'] += 1\n",
    "                elif 'Class' in node_type:\n",
    "                    features['class_count'] += 1\n",
    "                elif 'Import' in node_type:\n",
    "                    features['import_count'] += 1\n",
    "                elif 'Export' in node_type:\n",
    "                    features['export_count'] += 1\n",
    "                elif 'Variable' in node_type:\n",
    "                    features['variable_count'] += 1\n",
    "                \n",
    "                for value in node.values():\n",
    "                    if isinstance(value, (dict, list)):\n",
    "                        traverse(value)\n",
    "            elif isinstance(node, list):\n",
    "                for item in node:\n",
    "                    traverse(item)\n",
    "        \n",
    "        traverse(ast)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def compute_structural_similarity(project_metrics: List[Dict]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute structural similarity based on AST features.\n",
    "    \n",
    "    Args:\n",
    "        project_metrics: List of project metric dictionaries\n",
    "        \n",
    "    Returns:\n",
    "        NxN similarity matrix\n",
    "    \"\"\"\n",
    "    n = len(project_metrics)\n",
    "    \n",
    "    # Extract structural features for each project\n",
    "    project_features = []\n",
    "    \n",
    "    for metrics in project_metrics:\n",
    "        # Use metrics we already have\n",
    "        features = [\n",
    "            metrics['react_components'],\n",
    "            metrics['express_routes'],\n",
    "            metrics['mongoose_models'],\n",
    "            metrics['files_by_type'].get('.js', 0),\n",
    "            metrics['files_by_type'].get('.jsx', 0),\n",
    "            metrics['total_folders'],\n",
    "            metrics['loc'] / 1000  # Normalize LOC\n",
    "        ]\n",
    "        project_features.append(features)\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    features_matrix = np.array(project_features)\n",
    "    \n",
    "    # Normalize features\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    features_normalized = scaler.fit_transform(features_matrix)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarity_matrix = cosine_similarity(features_normalized)\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "print(\"✓ Structural similarity functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute structural similarity\n",
    "if project_metrics:\n",
    "    print(\"Computing structural similarity...\")\n",
    "    \n",
    "    structural_similarity = compute_structural_similarity(project_metrics)\n",
    "    print(f\"  Structural similarity matrix: {structural_similarity.shape}\")\n",
    "    \n",
    "    # Save matrix\n",
    "    np.save(os.path.join(RESULTS_DIR, 'structural_similarity_matrix.npy'), structural_similarity)\n",
    "    \n",
    "    # Save as CSV\n",
    "    structural_df = pd.DataFrame(structural_similarity, index=project_names, columns=project_names)\n",
    "    structural_df.to_csv(os.path.join(RESULTS_DIR, 'structural_similarity_matrix.csv'))\n",
    "    \n",
    "    print(\"✓ Structural similarity computed and saved\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(\"\\nSample (first 5x5):\")\n",
    "    print(structural_df.iloc[:5, :5].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Semantic Similarity (Using CodeBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "def compute_semantic_similarity(project_metrics: List[Dict]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute semantic similarity using CodeBERT embeddings.\n",
    "    \n",
    "    Args:\n",
    "        project_metrics: List of project metric dictionaries\n",
    "        \n",
    "    Returns:\n",
    "        NxN similarity matrix\n",
    "    \"\"\"\n",
    "    print(\"  Loading CodeBERT model...\")\n",
    "    \n",
    "    # Load CodeBERT model and tokenizer\n",
    "    model_name = \"microsoft/codebert-base\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"  Generating embeddings...\")\n",
    "    \n",
    "    # Generate embeddings for each project\n",
    "    embeddings = []\n",
    "    \n",
    "    for i, metrics in enumerate(project_metrics):\n",
    "        # Sample code from project (max 512 tokens)\n",
    "        code_sample = ' '.join(metrics['all_code'][:10])  # First 10 files\n",
    "        code_sample = code_sample[:2000]  # Limit length\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(code_sample, return_tensors=\"pt\", \n",
    "                          truncation=True, max_length=512, padding=True)\n",
    "        \n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            # Use [CLS] token embedding\n",
    "            embedding = outputs.last_hidden_state[:, 0, :].numpy().flatten()\n",
    "        \n",
    "        embeddings.append(embedding)\n",
    "        print(f\"    Project {i+1}/{len(project_metrics)} processed\")\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    embeddings_matrix = np.array(embeddings)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarity_matrix = cosine_similarity(embeddings_matrix)\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "print(\"✓ Semantic similarity function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute semantic similarity\n",
    "if project_metrics:\n",
    "    print(\"Computing semantic similarity (this may take a while)...\")\n",
    "    \n",
    "    try:\n",
    "        semantic_similarity = compute_semantic_similarity(project_metrics)\n",
    "        print(f\"  Semantic similarity matrix: {semantic_similarity.shape}\")\n",
    "        \n",
    "        # Save matrix\n",
    "        np.save(os.path.join(RESULTS_DIR, 'semantic_similarity_matrix.npy'), semantic_similarity)\n",
    "        \n",
    "        # Save as CSV\n",
    "        semantic_df = pd.DataFrame(semantic_similarity, index=project_names, columns=project_names)\n",
    "        semantic_df.to_csv(os.path.join(RESULTS_DIR, 'semantic_similarity_matrix.csv'))\n",
    "        \n",
    "        print(\"✓ Semantic similarity computed and saved\")\n",
    "        \n",
    "        # Show sample\n",
    "        print(\"\\nSample (first 5x5):\")\n",
    "        print(semantic_df.iloc[:5, :5].to_string())\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Error computing semantic similarity: {e}\")\n",
    "        print(\"  You may need to install: pip install torch transformers\")\n",
    "        semantic_similarity = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Identify Most and Least Similar Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_extreme_pairs(similarity_matrix: np.ndarray, project_names: List[str], \n",
    "                       metric_name: str, top_n: int = 5) -> None:\n",
    "    \"\"\"\n",
    "    Find and display most and least similar project pairs.\n",
    "    \n",
    "    Args:\n",
    "        similarity_matrix: NxN similarity matrix\n",
    "        project_names: List of project names\n",
    "        metric_name: Name of the similarity metric\n",
    "        top_n: Number of pairs to show\n",
    "    \"\"\"\n",
    "    n = len(project_names)\n",
    "    pairs = []\n",
    "    \n",
    "    # Get all pairs (excluding diagonal)\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            pairs.append({\n",
    "                'Project 1': project_names[i],\n",
    "                'Project 2': project_names[j],\n",
    "                'Similarity': similarity_matrix[i][j]\n",
    "            })\n",
    "    \n",
    "    # Sort by similarity\n",
    "    pairs_df = pd.DataFrame(pairs)\n",
    "    pairs_sorted = pairs_df.sort_values('Similarity', ascending=False)\n",
    "    \n",
    "    print(f\"\\n=== {metric_name.upper()} SIMILARITY ===\")\n",
    "    print(f\"\\nMost Similar Pairs (Top {top_n}):\")\n",
    "    print(pairs_sorted.head(top_n).to_string(index=False))\n",
    "    \n",
    "    print(f\"\\nLeast Similar Pairs (Bottom {top_n}):\")\n",
    "    print(pairs_sorted.tail(top_n).to_string(index=False))\n",
    "\n",
    "\n",
    "if project_metrics:\n",
    "    # Textual similarity pairs\n",
    "    find_extreme_pairs(textual_similarity, project_names, \"Textual\")\n",
    "    \n",
    "    # Structural similarity pairs\n",
    "    find_extreme_pairs(structural_similarity, project_names, \"Structural\")\n",
    "    \n",
    "    # Semantic similarity pairs (if available)\n",
    "    if semantic_similarity is not None:\n",
    "        find_extreme_pairs(semantic_similarity, project_names, \"Semantic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Part C: Visualization & Analysis <a name=\"part-c\"></a>\n",
    "\n",
    "### 4.1 Heatmap Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similarity_heatmap(similarity_matrix: np.ndarray, project_names: List[str],\n",
    "                           title: str, filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Create and save a heatmap visualization.\n",
    "    \n",
    "    Args:\n",
    "        similarity_matrix: NxN similarity matrix\n",
    "        project_names: List of project names\n",
    "        title: Plot title\n",
    "        filename: Output filename\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    sns.heatmap(similarity_matrix, \n",
    "                xticklabels=project_names,\n",
    "                yticklabels=project_names,\n",
    "                annot=True if len(project_names) <= 10 else False,\n",
    "                fmt='.2f',\n",
    "                cmap='YlOrRd',\n",
    "                vmin=0, vmax=1,\n",
    "                square=True,\n",
    "                cbar_kws={'label': 'Similarity Score'})\n",
    "    \n",
    "    plt.title(title, fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Projects', fontsize=12)\n",
    "    plt.ylabel('Projects', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    filepath = os.path.join(RESULTS_DIR, filename)\n",
    "    plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "    print(f\"  Saved: {filepath}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if project_metrics:\n",
    "    print(\"Creating heatmap visualizations...\\n\")\n",
    "    \n",
    "    # Textual similarity heatmap\n",
    "    plot_similarity_heatmap(textual_similarity, project_names,\n",
    "                           'Textual Similarity Matrix (TF-IDF + Cosine)',\n",
    "                           'textual_similarity_heatmap.png')\n",
    "    \n",
    "    # Structural similarity heatmap\n",
    "    plot_similarity_heatmap(structural_similarity, project_names,\n",
    "                           'Structural Similarity Matrix (AST Features)',\n",
    "                           'structural_similarity_heatmap.png')\n",
    "    \n",
    "    # Semantic similarity heatmap (if available)\n",
    "    if semantic_similarity is not None:\n",
    "        plot_similarity_heatmap(semantic_similarity, project_names,\n",
    "                               'Semantic Similarity Matrix (CodeBERT)',\n",
    "                               'semantic_similarity_heatmap.png')\n",
    "    \n",
    "    print(\"\\n✓ All heatmaps created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Comparison Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if project_metrics:\n",
    "    # Calculate average similarities (excluding diagonal)\n",
    "    def avg_similarity(matrix):\n",
    "        n = matrix.shape[0]\n",
    "        mask = ~np.eye(n, dtype=bool)\n",
    "        return matrix[mask].mean()\n",
    "    \n",
    "    avg_textual = avg_similarity(textual_similarity)\n",
    "    avg_structural = avg_similarity(structural_similarity)\n",
    "    \n",
    "    metrics_data = {\n",
    "        'Metric': ['Textual\\n(TF-IDF)', 'Structural\\n(AST Features)'],\n",
    "        'Average Similarity': [avg_textual, avg_structural]\n",
    "    }\n",
    "    \n",
    "    if semantic_similarity is not None:\n",
    "        avg_semantic = avg_similarity(semantic_similarity)\n",
    "        metrics_data['Metric'].append('Semantic\\n(CodeBERT)')\n",
    "        metrics_data['Average Similarity'].append(avg_semantic)\n",
    "    \n",
    "    # Create bar chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "    bars = plt.bar(metrics_data['Metric'], metrics_data['Average Similarity'], \n",
    "                   color=colors[:len(metrics_data['Metric'])], edgecolor='black', linewidth=1.5)\n",
    "    \n",
    "    plt.ylabel('Average Similarity Score', fontsize=12)\n",
    "    plt.xlabel('Similarity Metric', fontsize=12)\n",
    "    plt.title('Average Code Similarity by Metric', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    filepath = os.path.join(RESULTS_DIR, 'average_similarity_comparison.png')\n",
    "    plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved: {filepath}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Network Graph Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "if project_metrics:\n",
    "    # Create network graph based on textual similarity\n",
    "    # Only show edges with similarity > threshold\n",
    "    threshold = 0.3\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes\n",
    "    for name in project_names:\n",
    "        G.add_node(name)\n",
    "    \n",
    "    # Add edges for similar projects\n",
    "    n = len(project_names)\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            sim = textual_similarity[i][j]\n",
    "            if sim > threshold:\n",
    "                G.add_edge(project_names[i], project_names[j], weight=sim)\n",
    "    \n",
    "    # Plot network\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Layout\n",
    "    pos = nx.spring_layout(G, k=2, iterations=50, seed=42)\n",
    "    \n",
    "    # Draw nodes\n",
    "    nx.draw_networkx_nodes(G, pos, node_color='lightblue', \n",
    "                          node_size=1000, alpha=0.9, edgecolors='black', linewidths=2)\n",
    "    \n",
    "    # Draw edges with varying thickness based on similarity\n",
    "    edges = G.edges()\n",
    "    weights = [G[u][v]['weight'] for u, v in edges]\n",
    "    nx.draw_networkx_edges(G, pos, width=[w * 5 for w in weights], \n",
    "                          alpha=0.6, edge_color=weights, edge_cmap=plt.cm.YlOrRd)\n",
    "    \n",
    "    # Draw labels\n",
    "    nx.draw_networkx_labels(G, pos, font_size=8, font_weight='bold')\n",
    "    \n",
    "    plt.title(f'Project Similarity Network\\n(Edges shown for similarity > {threshold})',\n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    filepath = os.path.join(RESULTS_DIR, 'similarity_network.png')\n",
    "    plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved: {filepath}\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nNetwork statistics:\")\n",
    "    print(f\"  Nodes: {G.number_of_nodes()}\")\n",
    "    print(f\"  Edges: {G.number_of_edges()}\")\n",
    "    print(f\"  Density: {nx.density(G):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusions <a name=\"conclusions\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if project_metrics:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ANALYSIS SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\nTotal projects analyzed: {len(project_metrics)}\")\n",
    "    print(f\"\\nSimilarity metrics computed:\")\n",
    "    print(f\"  ✓ Textual (TF-IDF + Cosine Similarity)\")\n",
    "    print(f\"  ✓ Structural (AST Features)\")\n",
    "    if semantic_similarity is not None:\n",
    "        print(f\"  ✓ Semantic (CodeBERT Embeddings)\")\n",
    "    \n",
    "    print(f\"\\nAverage similarities:\")\n",
    "    print(f\"  Textual: {avg_textual:.3f}\")\n",
    "    print(f\"  Structural: {avg_structural:.3f}\")\n",
    "    if semantic_similarity is not None:\n",
    "        print(f\"  Semantic: {avg_semantic:.3f}\")\n",
    "    \n",
    "    print(f\"\\nOutputs saved in '{RESULTS_DIR}/' directory:\")\n",
    "    for file in sorted(os.listdir(RESULTS_DIR)):\n",
    "        print(f\"  - {file}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"KEY INSIGHTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\n1. CODING DIVERSITY:\")\n",
    "    if avg_textual < 0.3:\n",
    "        print(\"   → HIGH diversity: Projects show significant textual differences\")\n",
    "    elif avg_textual < 0.6:\n",
    "        print(\"   → MODERATE diversity: Some common patterns but varied implementations\")\n",
    "    else:\n",
    "        print(\"   → LOW diversity: Projects share substantial code similarities\")\n",
    "    \n",
    "    print(\"\\n2. STRUCTURAL CONSISTENCY:\")\n",
    "    if avg_structural > 0.7:\n",
    "        print(\"   → HIGH consistency: Teams followed similar architectural patterns\")\n",
    "    elif avg_structural > 0.4:\n",
    "        print(\"   → MODERATE consistency: Mix of different approaches\")\n",
    "    else:\n",
    "        print(\"   → LOW consistency: Diverse architectural choices\")\n",
    "    \n",
    "    print(\"\\n3. PATTERNS OF REUSE:\")\n",
    "    # Find projects with very high similarity\n",
    "    high_sim_count = np.sum(textual_similarity > 0.8) - len(project_names)  # Exclude diagonal\n",
    "    if high_sim_count > 0:\n",
    "        print(f\"   → Found {high_sim_count} project pairs with >80% similarity\")\n",
    "        print(\"   → Possible code sharing or template usage detected\")\n",
    "    else:\n",
    "        print(\"   → No significant code reuse patterns detected\")\n",
    "        print(\"   → Each team implemented independent solutions\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"✓ Analysis Complete!\")\n",
    "    print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
